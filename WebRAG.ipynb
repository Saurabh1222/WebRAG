{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSNrfaj019V38xLl6yh5RD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saurabh1222/WebRAG/blob/main/WebRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run in a notebook cell\n",
        "!pip uninstall -y opentelemetry-api opentelemetry-sdk opentelemetry-proto \\\n",
        "  opentelemetry-exporter-otlp-proto-common opentelemetry-exporter-otlp-proto-http\n",
        "\n",
        "!pip install opentelemetry-api==1.37.0 opentelemetry-sdk==1.37.0 \\\n",
        "  opentelemetry-proto==1.37.0 opentelemetry-exporter-otlp-proto-common==1.37.0 \\\n",
        "  opentelemetry-exporter-otlp-proto-http==1.37.0\n",
        "# now install our stack\n",
        "!pip install -q chromadb langchain sentence-transformers transformers huggingface-hub beautifulsoup4\n",
        "# verify\n",
        "!pip check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aGzgmdb3Xrmh",
        "outputId": "4a3f7552-bb3f-44ee-c32a-b0366eb05289"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opentelemetry-api 1.37.0\n",
            "Uninstalling opentelemetry-api-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-api-1.37.0\n",
            "Found existing installation: opentelemetry-sdk 1.37.0\n",
            "Uninstalling opentelemetry-sdk-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "Found existing installation: opentelemetry-proto 1.37.0\n",
            "Uninstalling opentelemetry-proto-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "Found existing installation: opentelemetry-exporter-otlp-proto-http 1.37.0\n",
            "Uninstalling opentelemetry-exporter-otlp-proto-http-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.37.0\n",
            "Collecting opentelemetry-api==1.37.0\n",
            "  Using cached opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk==1.37.0\n",
            "  Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-proto==1.37.0\n",
            "  Using cached opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0\n",
            "  Using cached opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.37.0\n",
            "  Using cached opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api==1.37.0) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api==1.37.0) (4.15.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk==1.37.0) (0.58b0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.37.0) (1.71.0)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.37.0) (2.32.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.37.0) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.37.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.37.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.37.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.37.0) (2025.10.5)\n",
            "Using cached opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "Using cached opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "Using cached opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Using cached opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: opentelemetry-proto, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-grpc 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.37.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-grpc 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.37.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-grpc 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.37.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-http-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mipython 7.34.0 requires jedi, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 has requirement opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 has requirement opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 has requirement opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0.\n",
            "google-colab 1.0.0 has requirement requests==2.32.4, but you have requests 2.32.5.\n",
            "google-adk 1.17.0 has requirement opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0.\n",
            "google-adk 1.17.0 has requirement opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q chromadb langchain sentence-transformers transformers huggingface-hub beautifulsoup4 langchain_community\n",
        "# verify\n",
        "!pip check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XzHNYgZpbanC",
        "outputId": "4e8eb408-ba98-4bb0-b143-6bf447e88a80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython 7.34.0 requires jedi, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 has requirement opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 has requirement opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 has requirement opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0.\n",
            "google-colab 1.0.0 has requirement requests==2.32.4, but you have requests 2.32.5.\n",
            "google-adk 1.17.0 has requirement opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0.\n",
            "google-adk 1.17.0 has requirement opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Imports + env check\n",
        "import os, requests, math\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import login as hf_login\n",
        "import torch"
      ],
      "metadata": {
        "id": "43ZEzVbAcquX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set HUGGINGFACE_HUB_TOKEN in env before running (recommended)\n",
        "from google.colab import userdata\n",
        "token = userdata.get('HF_TOKEN')\n",
        "if token:\n",
        "    hf_login(token=token)\n",
        "else:\n",
        "    raise RuntimeError(\"Set HUGGINGFACE_HUB_TOKEN in environment before running.\")"
      ],
      "metadata": {
        "id": "iVWMl4bkbcQA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Scraper (safe)\n",
        "def scrape_website(url, max_chars=200_000):\n",
        "    r = requests.get(url, timeout=15)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    text = \" \".join(p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\"))\n",
        "    return text[:max_chars]\n",
        "\n",
        "url = \"https://www.geeksforgeeks.org/artificial-intelligence/what-is-generative-ai/\"\n",
        "web_text = scrape_website(url)"
      ],
      "metadata": {
        "id": "kms2FOv6beCB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Chunking\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=web_text, metadata={\"source\": url})]\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
        "documents = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "3oJYttttblh4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Embeddings (batched) - use all-MiniLM-L6-v2\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
        "embedder = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# batch embed document texts (faster & safer)\n",
        "texts = [d.page_content for d in documents]\n",
        "batch_size = 32\n",
        "embeddings = []\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch = texts[i:i+batch_size]\n",
        "    batch_emb = embedder.embed_documents(batch)   # returns list of vectors\n",
        "    embeddings.extend(batch_emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQgGNVkIbqff",
        "outputId": "af836928-62a6-4ba6-abac-ff2c8cfd188a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3161282310.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedder = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Persist Chroma vectorstore (use Drive in Colab for persistence)\n",
        "from langchain.vectorstores import Chroma\n",
        "persist_dir = \"./chromadb_persist\"  # change to '/content/drive/MyDrive/... ' for Drive persistence\n",
        "vectorstore = Chroma.from_documents(documents, embedder, persist_directory=persist_dir)\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXYlqkyMbtPw",
        "outputId": "97329bba-4f6d-4829-faad-22cf6f700ddb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-963104781.py:5: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Choose free HF API LLM (google/flan-t5-base)\n",
        "#    We use LangChain's HuggingFacePipeline wrapper which uses the transformers pipeline.\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "\n",
        "hf_model = \"google/flan-t5-base\"   # or \"google/flan-t5-small\" for cheaper & faster dev\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(hf_model)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BNtk6-ebvFK",
        "outputId": "bacf9680-a7fe-4d7f-819d-0c05758b9dd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "/tmp/ipython-input-677090556.py:19: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) RetrievalQA chain using the LLM + Chroma retriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})  # top-k retrieval\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"Answer the question using only the context below. Be short and accurate.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        ")\n",
        "\n",
        "rag = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "XiMa44aTb1yP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Query example\n",
        "query = \"What is Gen AI?\"\n",
        "out = rag({\"query\": query})\n",
        "\n",
        "print(\"=== ANSWER ===\")\n",
        "print(out[\"result\"].strip())\n",
        "print(\"\\n=== SOURCES (first chunk) ===\")\n",
        "if out.get(\"source_documents\"):\n",
        "    print(out[\"source_documents\"][0].page_content[:600].strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W78KAUyOawLp",
        "outputId": "97799e9c-7b76-4dfe-ae40-acbe1e671ac2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1922422975.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  out = rag({\"query\": query})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANSWER ===\n",
            "Generative AI is trained on large datasets like text, images, audio or video using deep learning networks. During training, the model learns parameters (millions or billions of them) that help them predict or generate content. Here models generate output based on learned patterns and prompts provided Modern systems often uses agents which are autonomous components that interact with the environment, obtain information and execute chains of tasks. These agents uses LLMs to reason, plan and act enabling workflows like querying databases, outputs by retrieving relevant documents at query time to ground the generation in accurate, up-to-date\n",
            "\n",
            "=== SOURCES (first chunk) ===\n",
            "audio or video that resembles real-world examples. Generative AI is trained on large datasets like text, images, audio or video using deep learning networks. During training, the model learns parameters (millions or billions of them) that help them predict or generate content. Here models generate output based on learned patterns and prompts provided Modern systems often uses agents which are autonomous components that interact with the environment, obtain information and execute chains of tasks. These agents uses LLMs to reason, plan and act enabling workflows like querying databases,\n"
          ]
        }
      ]
    }
  ]
}